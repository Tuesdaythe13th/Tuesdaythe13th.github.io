---
title: 'MLCommons Security Jailbreak Benchmark v0.5'

authors:
  - MLCommons Security Working Group
  - me

date: '2025-01-01T00:00:00Z'

publishDate: '2025-01-01T00:00:00Z'

publication_types: ['report']

publication: MLCommons Security Working Group
publication_short: MLCommons Security WG

abstract: The MLCommons Security Jailbreak Benchmark v0.5 provides a standardized framework for evaluating AI system robustness against adversarial attacks and jailbreak attempts. This work introduces the novel "Resilience Gap" metric, which quantifies the difference between baseline safety performance and performance under adversarial conditions. The benchmark informs ISO/IEC 42001 safety standards and provides critical security evaluation capabilities for frontier AI systems.

summary: Standardized jailbreak robustness benchmark introducing the Resilience Gap metric for measuring AI security under adversarial attack. Informs ISO/IEC 42001 standards.

tags:
  - AI Security
  - Red Teaming
  - Jailbreak
  - Adversarial Robustness
  - Benchmark
  - MLCommons
  - Standards

featured: true

links:
  - type: pdf
    url: https://mlcommons.org/
  - type: source
    url: https://mlcommons.org/

image:
  caption: 'Security jailbreak evaluation framework'
  focal_point: ''
  preview_only: false

projects: []

slides: ""
---

The MLCommons Security Jailbreak Benchmark v0.5 establishes a critical foundation for evaluating AI system security under adversarial conditions. Co-authored with the MLCommons Security Working Group, this benchmark introduces the Resilience Gap metricâ€”a novel measure that quantifies how much an AI system's safety guarantees degrade under attack.

This work directly informs international safety standards including ISO/IEC 42001 and provides organizations with practical tools for assessing the robustness of their AI deployments. The benchmark is available via MLCommons whitepaper at mlcommons.org.
