---
title: 'UNESCO Red Teaming Playbook: Tackling Gender Bias and Harms in Artificial Intelligence'

authors:
  - UNESCO
  - Humane Intelligence
  - me

date: '2024-01-01T00:00:00Z'

publishDate: '2024-01-01T00:00:00Z'

publication_types: ['report']

publication: United Nations Educational, Scientific and Cultural Organization (UNESCO)
publication_short: UNESCO

abstract: The UNESCO Red Teaming Playbook provides global guidance for tackling gender bias and harms in artificial intelligence systems. This playbook synthesizes research and methodologies for conducting effective red teaming assessments, with focus on identifying and mitigating bias in AI systems deployed globally. Research contributions through Humane Intelligence inform best practices for multimodal red teaming, bias detection, and adversarial evaluation methodologies.

summary: Research contributor to UNESCO's global AI red teaming playbook, providing guidance on bias detection and adversarial evaluation for AI-for-social-good applications.

tags:
  - AI Safety
  - Red Teaming
  - Bias Detection
  - Gender Bias
  - UNESCO
  - Social Impact
  - Policy

featured: true

links:
  - type: pdf
    url: https://www.unesco.org/

image:
  caption: 'UNESCO Red Teaming Playbook'
  focal_point: ''
  preview_only: false

projects: []

slides: ""
---

This work represents contributions to UNESCO's global guidance on AI safety and bias mitigation through red teaming methodologies. Through collaboration with Humane Intelligence, research on multimodal red teaming, bias detection in computer vision systems, and adversarial evaluation techniques was incorporated into this comprehensive playbook for international AI practitioners.

The playbook serves as a practical resource for organizations worldwide implementing AI systems with attention to gender bias and social harms, with particular applicability to AI-for-social-good initiatives and public interest technology.
