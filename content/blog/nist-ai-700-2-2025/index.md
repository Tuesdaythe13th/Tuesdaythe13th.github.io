---
title: Research Cited in NIST AI 700-2 Trustworthy & Responsible AI Report
summary: Red teaming research with Humane Intelligence contributes to NIST's pilot evaluation for AI trustworthiness standards.
date: 2025-01-20

image:
  caption: 'NIST AI Safety Standards'

authors:
  - me

tags:
  - AI Safety
  - Red Teaming
  - Policy
  - NIST
  - Standards
  - Trustworthy AI

content_meta:
  trending: true
---

Excited to share that red teaming research conducted with [Humane Intelligence](https://www.humaneintelligence.org/) has been cited in the NIST AI 700-2 report on Trustworthy and Responsible AI. This work contributed to the ARIA 0.1 pilot evaluation, helping to establish standardized methodologies for evaluating AI trustworthiness at the federal level.

## About NIST AI 700-2

The National Institute of Standards and Technology (NIST) plays a critical role in developing AI safety standards and evaluation methodologies for the United States government. The AI 700-2 report establishes frameworks for assessing whether AI systems meet trustworthiness criteria across multiple dimensions including safety, security, fairness, and transparency.

## Our Contribution

Through collaboration with Humane Intelligence, our red teaming methodologies and multimodal safety assessment techniques were incorporated into the ARIA 0.1 pilot evaluation. This work demonstrates how adversarial evaluation and bias detection research can inform federal AI safety policy.

Key areas of contribution:
- **Multimodal Red Teaming**: Methodologies for testing AI systems across text, image, and video modalities
- **Bias Detection Frameworks**: Techniques for identifying subtle biases in computer vision and language systems
- **Adversarial Robustness**: Approaches for evaluating system behavior under adversarial conditions

## Impact

This recognition validates the importance of independent red teaming research in shaping national AI safety standards. As AI systems become increasingly deployed in critical infrastructure and government applications, rigorous evaluation methodologies become essential for public trust and safety.

The work continues through ongoing collaborations with policy institutions, standards bodies, and public interest organizations to ensure AI deployments meet trustworthiness requirements.

---

**Related Work**:
- [Humane Intelligence Bias Bounty 2](https://www.humaneintelligence.org/)
- [MLCommons Security Working Group](/publications/mlcommons-security-jailbreak/)
- [UNESCO Red Teaming Playbook](/publications/unesco-red-teaming-playbook/)
